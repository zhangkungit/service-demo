<?xml version="1.0" ?>

<!-- 属性描述 scan：性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true scanPeriod:设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。
    debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 -->
<configuration scan="true" scanPeriod="600 seconds" debug="false">
    <!-- 定义日志文件 输入位置 -->
    <property name="log_dir" value="/applog/service-demo/user/${HOSTNAME}"/>
    <!-- 日志最大的历史-->
    <property name="maxHistory" value="90"/>
    <property name="normal-pattern" value="%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger - %msg%n"/>

    <!-- ConsoleAppender 控制台输出日志 -->
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <!-- 对日志进行格式化 -->
        <encoder>
            <pattern>${normal-pattern}</pattern>
        </encoder>
    </appender>

    <!-- ERROR级别日志 -->
    <!-- 滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件 RollingFileAppender-->
    <appender name="ERROR" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 过滤器，只记录WARN级别的日志 -->
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>ERROR</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <file>${log_dir}/error.log</file>
        <!-- 配置日志所生成的目录-->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log_dir}/error-%d{yyyy-MM-dd-HH}.log.gz</fileNamePattern>
            <maxHistory>${maxHistory}</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>${normal-pattern}</pattern>
        </encoder>
    </appender>

    <!-- INFO级别日志 appender -->
    <appender name="INFO" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <file>${log_dir}/info.log</file>
        <!-- 配置日志所生成的目录-->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log_dir}/info-%d{yyyy-MM-dd-HH}.log.gz</fileNamePattern>
            <maxHistory>${maxHistory}</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>${normal-pattern}</pattern>
        </encoder>
    </appender>

    <!-- DEBUG级别日志 appender -->
    <appender name="DEBUG" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>DEBUG</level>
        </filter>
        <file>${log_dir}/debug.log</file>
        <!-- 配置日志所生成的目录-->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log_dir}/debug-%d{yyyy-MM-dd-HH}.log.gz</fileNamePattern>
            <maxHistory>${maxHistory}</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>${normal-pattern}</pattern>
        </encoder>
    </appender>

    <!-- This is the kafkaAppender -->
    <!--<appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        &lt;!&ndash; This is the default encoder that encodes every log message to an utf8-encoded string  &ndash;&gt;
        <encoder class="com.github.danielwegener.logback.kafka.encoding.LayoutKafkaMessageEncoder">
            <layout class="ch.qos.logback.classic.PatternLayout">
                <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>
            </layout>
        </encoder>
        <topic>test</topic>
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.RoundRobinKeyingStrategy" />
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy" />

        &lt;!&ndash; each <producerConfig> translates to regular kafka-client config (format: key=value) &ndash;&gt;
        &lt;!&ndash; producer configs are documented here: https://kafka.apache.org/documentation.html#newproducerconfigs &ndash;&gt;
        &lt;!&ndash; bootstrap.servers is the only mandatory producerConfig &ndash;&gt;
        <producerConfig>bootstrap.servers=192.168.64.128:9092</producerConfig>

        &lt;!&ndash; this is the fallback appender if kafka is not available. &ndash;&gt;
        &lt;!&ndash;<appender-ref ref="STDOUT">&ndash;&gt;
    </appender>-->

    <!-- rabbit mq  -->
    <logger name="org.springframework.amqp.rabbit">
        <level value="WARN"/>
    </logger>
    <!--zookeeper-->
    <logger name="org.apache.zookeeper">
        <level value="WARN"/>
    </logger>
    <!--mongodb-->
    <logger name="org.mongodb.driver">
        <level value="WARN"/>
    </logger>
    <!--quartz-->
    <logger name="org.quartz.core">
        <level value="WARN"/>
    </logger>




    <!-- root级别   DEBUG -->
    <root level="DEBUG">
        <!-- 控制台输出 -->
        <appender-ref ref="STDOUT" />
        <!-- 文件输出 -->
        <appender-ref ref="ERROR" />
        <appender-ref ref="INFO" />
        <appender-ref ref="DEBUG" />
        <!--<appender-ref ref="kafkaAppender" />-->
    </root>
</configuration>